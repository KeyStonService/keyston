#!/usr/bin/env python3

"""
Enhanced Root Layer Validator
å¢å¼ºæ ¹å±‚éªŒè¯å™¨ - åŒ…å«è·¨æ–‡ä»¶ä¸€è‡´æ€§æ£€æŸ¥ã€æ™ºèƒ½ä¿®å¤å»ºè®®ã€æ–°å¢æ–‡ä»¶éªŒè¯
"""

from __future__ import annotations

import os
import sys
import yaml
import json
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Set, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict


@dataclass
class ValidationIssue:
    """éªŒè¯é—®é¢˜"""
    severity: str  # critical, high, medium, low, info
    category: str  # schema, consistency, reference, dependency, best_practice
    file_path: str
    line_number: Optional[int]
    message: str
    suggestion: Optional[str]
    auto_fixable: bool
    related_files: List[str]


@dataclass
class FileMetrics:
    """æ–‡ä»¶æŒ‡æ ‡"""
    file_path: str
    file_type: str
    size_kb: float
    entity_count: int
    reference_count: int
    dependency_count: int
    complexity_score: int
    quality_score: int


class EnhancedRootValidator:
    """å¢å¼ºæ ¹å±‚éªŒè¯å™¨"""
    
    def __init__(self, workspace_root: str = None):
        if workspace_root is None:
            workspace_root = os.environ.get("MACHINENATIVEOPS_WORKSPACE")
        if workspace_root is None:
            workspace_root = Path(__file__).resolve().parents[3]
        
        self.workspace_root = Path(workspace_root)
        self.baseline_root = self.workspace_root / "controlplane" / "baseline"
        self.overlay_root = self.workspace_root / "controlplane" / "overlay"
        self.evidence_root = self.overlay_root / "evidence" / "validation"
        self.registry_root = self.baseline_root / "registries"
        self.specs_root = self.baseline_root / "specifications"
        self.config_root = self.baseline_root / "config"
        
        self.results = {
            "validation_id": self._generate_validation_id(),
            "timestamp": datetime.utcnow().isoformat(),
            "workspace": str(self.workspace_root),
            "stages": {},
            "summary": {
                "total_checks": 0,
                "passed": 0,
                "failed": 0,
                "warnings": 0,
                "info": 0
            },
            "issues": [],
            "metrics": {},
            "pass": False
        }
        
        # ç¡®ä¿è¯æ®ç›®å½•å­˜åœ¨
        self.evidence_root.mkdir(parents=True, exist_ok=True)
        
    def _generate_validation_id(self) -> str:
        """ç”ŸæˆéªŒè¯ID"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        return f"enhanced_validation_{timestamp}"
    
    def _load_yaml(self, path: Path) -> Optional[Dict[str, Any]]:
        """å®‰å…¨åŠ è½½YAMLæ–‡ä»¶"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        except (yaml.YAMLError, FileNotFoundError, UnicodeDecodeError) as e:
            return None
    
    def _calculate_file_hash(self, path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        try:
            content = path.read_text(encoding='utf-8')
            return hashlib.sha256(content.encode()).hexdigest()
        except (OSError, UnicodeDecodeError):
            return "unavailable"
    
    def validate_schema_compliance(self) -> List[ValidationIssue]:
        """éªŒè¯æ¨¡å¼åˆè§„æ€§"""
        issues = []
        
        # å®šä¹‰å„ç±»å‹æ–‡ä»¶çš„æ¨¡å¼
        schemas = self._load_validation_schemas()
        
        for file_path in self.config_root.glob("root.*.yaml"):
            file_type = self._determine_file_type(file_path.name)
            
            if file_type in schemas:
                schema = schemas[file_type]
                content = self._load_yaml(file_path)
                
                if content is None:
                    issues.append(ValidationIssue(
                        severity="critical",
                        category="schema",
                        file_path=str(file_path.relative_to(self.workspace_root)),
                        line_number=None,
                        message="æ— æ³•è§£æYAMLæ–‡ä»¶",
                        suggestion="æ£€æŸ¥YAMLè¯­æ³•å’Œæ–‡ä»¶ç¼–ç ",
                        auto_fixable=False,
                        related_files=[]
                    ))
                    continue
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                for required_field in schema.get("required_fields", []):
                    if required_field not in content:
                        issues.append(ValidationIssue(
                            severity="high",
                            category="schema",
                            file_path=str(file_path.relative_to(self.workspace_root)),
                            line_number=None,
                            message=f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {required_field}",
                            suggestion=f"æ·»åŠ å­—æ®µ: {required_field}: <value>",
                            auto_fixable=True,
                            related_files=[]
                        ))
                
                # éªŒè¯å­—æ®µç±»å‹
                for field_name, field_schema in schema.get("fields", {}).items():
                    if field_name in content:
                        expected_type = field_schema.get("type")
                        actual_value = content[field_name]
                        
                        if not self._validate_field_type(actual_value, expected_type):
                            issues.append(ValidationIssue(
                                severity="medium",
                                category="schema",
                                file_path=str(file_path.relative_to(self.workspace_root)),
                                line_number=None,
                                message=f"å­—æ®µç±»å‹ä¸åŒ¹é…: {field_name} åº”ä¸º {expected_type}",
                                suggestion=f"å°† {field_name} çš„å€¼è½¬æ¢ä¸º {expected_type} ç±»å‹",
                                auto_fixable=False,
                                related_files=[]
                            ))
        
        return issues
    
    def validate_cross_file_consistency(self) -> List[ValidationIssue]:
        """éªŒè¯è·¨æ–‡ä»¶ä¸€è‡´æ€§"""
        issues = []
        
        # åŠ è½½æ‰€æœ‰é…ç½®æ–‡ä»¶
        all_configs = {}
        for config_file in self.config_root.glob("root.*.yaml"):
            content = self._load_yaml(config_file)
            if content:
                all_configs[config_file.name] = {
                    "path": config_file,
                    "content": content
                }
        
        # æ£€æŸ¥ç‰ˆæœ¬ä¸€è‡´æ€§
        versions = {}
        for file_name, config_info in all_configs.items():
            content = config_info["content"]
            if "version" in content:
                versions[file_name] = content["version"]
        
        if len(set(versions.values())) > 1:
            issues.append(ValidationIssue(
                severity="medium",
                category="consistency",
                file_path="multiple",
                line_number=None,
                message="å‘ç°ç‰ˆæœ¬ä¸ä¸€è‡´",
                suggestion=f"ç»Ÿä¸€æ‰€æœ‰é…ç½®æ–‡ä»¶çš„ç‰ˆæœ¬å·ï¼Œå½“å‰ç‰ˆæœ¬: {versions}",
                auto_fixable=True,
                related_files=list(versions.keys())
            ))
        
        # æ£€æŸ¥æ—¶é—´æˆ³ä¸€è‡´æ€§
        timestamps = {}
        for file_name, config_info in all_configs.items():
            content = config_info["content"]
            timestamp_fields = ["created", "updated", "last_modified"]
            for field in timestamp_fields:
                if field in content:
                    timestamps[f"{file_name}:{field}"] = content[field]
        
        # æ£€æŸ¥å‘½åè§„èŒƒä¸€è‡´æ€§
        naming_patterns = {}
        for spec_file in self.specs_root.glob("root.specs.*.yaml"):
            content = self._load_yaml(spec_file)
            if content and "patterns" in content:
                naming_patterns[spec_file.name] = content["patterns"]
        
        # éªŒè¯å®é™…æ–‡ä»¶åæ˜¯å¦ç¬¦åˆå‘½åè§„èŒƒ
        for config_file in self.config_root.glob("root.*.yaml"):
            file_name = config_file.name
            if "naming" in naming_patterns:
                patterns = naming_patterns["naming"].get("file_patterns", {})
                for pattern_name, pattern_regex in patterns.items():
                    if not re.match(pattern_regex, file_name):
                        issues.append(ValidationIssue(
                            severity="medium",
                            category="consistency",
                            file_path=str(config_file.relative_to(self.workspace_root)),
                            line_number=None,
                            message=f"æ–‡ä»¶åä¸ç¬¦åˆå‘½åè§„èŒƒ: {pattern_name}",
                            suggestion=f"å°†æ–‡ä»¶åä¿®æ”¹ä¸ºç¬¦åˆæ¨¡å¼çš„æ ¼å¼",
                            auto_fixable=False,
                            related_files=["controlplane/baseline/specifications/root.specs.naming.yaml"]
                        ))
        
        return issues
    
    def validate_reference_integrity(self) -> List[ValidationIssue]:
        """éªŒè¯å¼•ç”¨å®Œæ•´æ€§"""
        issues = []
        
        # æ”¶é›†æ‰€æœ‰å¯ç”¨çš„URN
        available_urns = set()
        
        # ä»æ³¨å†Œè¡¨æ”¶é›†URN
        for registry_file in self.registry_root.glob("root.registry.*.yaml"):
            content = self._load_yaml(registry_file)
            if content and "entries" in content:
                for entry in content["entries"]:
                    if "urn" in entry:
                        available_urns.add(entry["urn"])
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„URNå¼•ç”¨
        for config_file in self.config_root.glob("root.*.yaml"):
            content = self._load_yaml(config_file)
            if content:
                referenced_urns = self._extract_urns(content)
                
                for urn in referenced_urns:
                    if urn not in available_urns:
                        issues.append(ValidationIssue(
                            severity="high",
                            category="reference",
                            file_path=str(config_file.relative_to(self.workspace_root)),
                            line_number=None,
                            message=f"å¼•ç”¨çš„URNä¸å­˜åœ¨: {urn}",
                            suggestion=f"åœ¨ç›¸åº”çš„æ³¨å†Œè¡¨ä¸­åˆ›å»ºURNæ¡ç›®æˆ–æ£€æŸ¥å¼•ç”¨æ˜¯å¦æ­£ç¡®",
                            auto_fixable=False,
                            related_files=self._find_registry_files_for_urn(urn)
                        ))
        
        # æ£€æŸ¥æ–‡ä»¶å†…éƒ¨å¼•ç”¨
        for config_file in self.config_root.glob("root.*.yaml"):
            content = self._load_yaml(config_file)
            if content:
                file_references = self._extract_file_references(content)
                
                for ref_file in file_references:
                    ref_path = self.workspace_root / ref_file
                    if not ref_path.exists():
                        issues.append(ValidationIssue(
                            severity="medium",
                            category="reference",
                            file_path=str(config_file.relative_to(self.workspace_root)),
                            line_number=None,
                            message=f"å¼•ç”¨çš„æ–‡ä»¶ä¸å­˜åœ¨: {ref_file}",
                            suggestion=f"åˆ›å»ºæ–‡ä»¶ {ref_file} æˆ–ä¿®å¤å¼•ç”¨è·¯å¾„",
                            auto_fixable=False,
                            related_files=[ref_file]
                        ))
        
        return issues
    
    def validate_dependency_graph(self) -> List[ValidationIssue]:
        """éªŒè¯ä¾èµ–å›¾"""
        issues = []
        
        # æ„å»ºä¾èµ–å›¾
        dependency_graph = defaultdict(set)
        all_files = set()
        
        # æ”¶é›†æ‰€æœ‰é…ç½®æ–‡ä»¶
        config_files = list(self.config_root.glob("root.*.yaml"))
        spec_files = list(self.specs_root.glob("root.specs.*.yaml"))
        registry_files = list(self.registry_root.glob("root.registry.*.yaml"))
        
        all_files.update([f.name for f in config_files])
        all_files.update([f.name for f in spec_files])
        all_files.update([f.name for f in registry_files])
        
        # åˆ†æä¾èµ–å…³ç³»
        for file_path in config_files + spec_files + registry_files:
            content = self._load_yaml(file_path)
            if content:
                file_name = file_path.name
                dependencies = self._extract_dependencies(content)
                
                for dep in dependencies:
                    if dep in all_files:
                        dependency_graph[file_name].add(dep)
        
        # æ£€æŸ¥å¾ªç¯ä¾èµ–
        visited = set()
        rec_stack = set()
        
        def has_cycle(node):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in dependency_graph[node]:
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True
            
            rec_stack.remove(node)
            return False
        
        for file_name in all_files:
            if file_name not in visited:
                if has_cycle(file_name):
                    issues.append(ValidationIssue(
                        severity="high",
                        category="dependency",
                        file_path=file_name,
                        line_number=None,
                        message=f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œæ¶‰åŠæ–‡ä»¶: {file_name}",
                        suggestion="é‡æ„ä¾èµ–å…³ç³»ä»¥æ¶ˆé™¤å¾ªç¯",
                        auto_fixable=False,
                        related_files=list(dependency_graph[file_name])
                    ))
        
        # æ£€æŸ¥ç¼ºå¤±çš„ä¾èµ–
        for file_name, deps in dependency_graph.items():
            for dep in deps:
                if dep not in all_files:
                    issues.append(ValidationIssue(
                        severity="medium",
                        category="dependency",
                        file_path=file_name,
                        line_number=None,
                        message=f"ä¾èµ–çš„æ–‡ä»¶ä¸å­˜åœ¨: {dep}",
                        suggestion=f"åˆ›å»ºç¼ºå¤±çš„æ–‡ä»¶æˆ–ç§»é™¤ä¾èµ–",
                        auto_fixable=False,
                        related_files=[dep]
                    ))
        
        return issues
    
    def validate_new_files(self) -> List[ValidationIssue]:
        """éªŒè¯æ–°å¢æ–‡ä»¶"""
        issues = []
        
        # éªŒè¯æ–°å¢çš„è§„èŒƒæ–‡ä»¶
        new_spec_files = [
            "root.specs.namespace.yaml",
            "root.specs.paths.yaml", 
            "root.specs.urn.yaml"
        ]
        
        for spec_file in new_spec_files:
            spec_path = self.specs_root / spec_file
            if spec_path.exists():
                content = self._load_yaml(spec_path)
                if not content:
                    continue
                
                # éªŒè¯namespaceè§„èŒƒ
                if "namespace" in spec_file:
                    required_fields = ["namespaces", "hierarchy", "validation_rules"]
                    for field in required_fields:
                        if field not in content:
                            issues.append(ValidationIssue(
                                severity="high",
                                category="schema",
                                file_path=str(spec_path.relative_to(self.workspace_root)),
                                line_number=None,
                                message=f"namespaceè§„èŒƒç¼ºå°‘å¿…éœ€å­—æ®µ: {field}",
                                suggestion=f"æ·»åŠ  {field} å®šä¹‰",
                                auto_fixable=True,
                                related_files=[]
                            ))
                
                # éªŒè¯pathsè§„èŒƒ
                elif "paths" in spec_file:
                    required_fields = ["path_patterns", "validation_rules", "mapping_rules"]
                    for field in required_fields:
                        if field not in content:
                            issues.append(ValidationIssue(
                                severity="high",
                                category="schema",
                                file_path=str(spec_path.relative_to(self.workspace_root)),
                                line_number=None,
                                message=f"pathsè§„èŒƒç¼ºå°‘å¿…éœ€å­—æ®µ: {field}",
                                suggestion=f"æ·»åŠ  {field} å®šä¹‰",
                                auto_fixable=True,
                                related_files=[]
                            ))
                
                # éªŒè¯URNè§„èŒƒ
                elif "urn" in spec_file:
                    required_fields = ["urn_format", "namespace_rules", "validation_rules"]
                    for field in required_fields:
                        if field not in content:
                            issues.append(ValidationIssue(
                                severity="high",
                                category="schema",
                                file_path=str(spec_path.relative_to(self.workspace_root)),
                                line_number=None,
                                message=f"URNè§„èŒƒç¼ºå°‘å¿…éœ€å­—æ®µ: {field}",
                                suggestion=f"æ·»åŠ  {field} å®šä¹‰",
                                auto_fixable=True,
                                related_files=[]
                            ))
        
        # éªŒè¯æ–°å¢çš„æ³¨å†Œè¡¨æ–‡ä»¶
        new_registry_files = [
            "root.registry.devices.yaml",
            "root.registry.namespaces.yaml"
        ]
        
        for registry_file in new_registry_files:
            registry_path = self.registry_root / registry_file
            if registry_path.exists():
                content = self._load_yaml(registry_path)
                if not content:
                    continue
                
                # éªŒè¯æ³¨å†Œè¡¨ç»“æ„
                if "entries" not in content:
                    issues.append(ValidationIssue(
                        severity="high",
                        category="schema",
                        file_path=str(registry_path.relative_to(self.workspace_root)),
                        line_number=None,
                        message="æ³¨å†Œè¡¨æ–‡ä»¶ç¼ºå°‘entrieså­—æ®µ",
                        suggestion="æ·»åŠ entriesæ•°ç»„å®šä¹‰",
                        auto_fixable=True,
                        related_files=[]
                    ))
                else:
                    entries = content["entries"]
                    if not isinstance(entries, list):
                        issues.append(ValidationIssue(
                            severity="high",
                            category="schema",
                            file_path=str(registry_path.relative_to(self.workspace_root)),
                            line_number=None,
                            message="entrieså­—æ®µå¿…é¡»æ˜¯æ•°ç»„ç±»å‹",
                            suggestion="å°†entriesæ”¹ä¸ºæ•°ç»„æ ¼å¼",
                            auto_fixable=True,
                            related_files=[]
                        ))
                    else:
                        for i, entry in enumerate(entries):
                            if not isinstance(entry, dict):
                                issues.append(ValidationIssue(
                                    severity="medium",
                                    category="schema",
                                    file_path=str(registry_path.relative_to(self.workspace_root)),
                                    line_number=None,
                                    message=f"entries[{i}] å¿…é¡»æ˜¯å¯¹è±¡ç±»å‹",
                                    suggestion="å°†æ¡ç›®æ”¹ä¸ºå¯¹è±¡æ ¼å¼",
                                    auto_fixable=True,
                                    related_files=[]
                                ))
        
        # éªŒè¯gates.map.yaml
        gates_file = self.config_root / "gates.map.yaml"
        if gates_file.exists():
            content = self._load_yaml(gates_file)
            if content:
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ["version", "gates", "execution_order"]
                for field in required_fields:
                    if field not in content:
                        issues.append(ValidationIssue(
                            severity="high",
                            category="schema",
                            file_path=str(gates_file.relative_to(self.workspace_root)),
                            line_number=None,
                            message=f"gates.map.yamlç¼ºå°‘å¿…éœ€å­—æ®µ: {field}",
                            suggestion=f"æ·»åŠ  {field} å®šä¹‰",
                            auto_fixable=True,
                            related_files=[]
                        ))
                
                # éªŒè¯gateå®šä¹‰
                if "gates" in content:
                    gates = content["gates"]
                    for gate_name, gate_config in gates.items():
                        if not isinstance(gate_config, dict):
                            issues.append(ValidationIssue(
                                severity="medium",
                                category="schema",
                                file_path=str(gates_file.relative_to(self.workspace_root)),
                                line_number=None,
                                message=f"gate '{gate_name}' é…ç½®å¿…é¡»æ˜¯å¯¹è±¡ç±»å‹",
                                suggestion="å°†gateé…ç½®æ”¹ä¸ºå¯¹è±¡æ ¼å¼",
                                auto_fixable=True,
                                related_files=[]
                            ))
                        else:
                            required_gate_fields = ["enabled", "description"]
                            for field in required_gate_fields:
                                if field not in gate_config:
                                    issues.append(ValidationIssue(
                                        severity="medium",
                                        category="schema",
                                        file_path=str(gates_file.relative_to(self.workspace_root)),
                                        line_number=None,
                                        message=f"gate '{gate_name}' ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}",
                                        suggestion=f"æ·»åŠ  {field} å®šä¹‰",
                                        auto_fixable=True,
                                        related_files=[]
                                    ))
        
        return issues
    
    def calculate_file_metrics(self) -> Dict[str, FileMetrics]:
        """è®¡ç®—æ–‡ä»¶æŒ‡æ ‡"""
        metrics = {}
        
        all_files = []
        all_files.extend(self.config_root.glob("root.*.yaml"))
        all_files.extend(self.specs_root.glob("root.specs.*.yaml"))
        all_files.extend(self.registry_root.glob("root.registry.*.yaml"))
        
        for file_path in all_files:
            content = self._load_yaml(file_path)
            if not content:
                continue
            
            file_size_kb = file_path.stat().st_size / 1024
            entity_count = len(self._extract_entities(content))
            reference_count = len(self._extract_urns(content)) + len(self._extract_file_references(content))
            dependency_count = len(self._extract_dependencies(content))
            
            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = self._calculate_complexity(content)
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = self._calculate_quality_score(content, file_path.name)
            
            metrics[str(file_path.relative_to(self.workspace_root))] = FileMetrics(
                file_path=str(file_path.relative_to(self.workspace_root)),
                file_type=self._determine_file_type(file_path.name),
                size_kb=round(file_size_kb, 2),
                entity_count=entity_count,
                reference_count=reference_count,
                dependency_count=dependency_count,
                complexity_score=complexity_score,
                quality_score=quality_score
            )
        
        return metrics
    
    def generate_enhanced_report(self) -> str:
        """ç”Ÿæˆå¢å¼ºæŠ¥å‘Š"""
        issues = []
        
        # æ‰§è¡Œæ‰€æœ‰éªŒè¯
        issues.extend(self.validate_schema_compliance())
        issues.extend(self.validate_cross_file_consistency())
        issues.extend(self.validate_reference_integrity())
        issues.extend(self.validate_dependency_graph())
        issues.extend(self.validate_new_files())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_file_metrics()
        
        # æ›´æ–°ç»“æœ
        self.results["issues"] = [asdict(issue) for issue in issues]
        self.results["metrics"] = {k: asdict(v) for k, v in metrics.items()}
        
        # ç»Ÿè®¡
        self.results["summary"]["total_checks"] = len(issues)
        self.results["summary"]["failed"] = len([i for i in issues if i["severity"] in ["critical", "high"]])
        self.results["summary"]["warnings"] = len([i for i in issues if i["severity"] == "medium"])
        self.results["summary"]["info"] = len([i for i in issues if i["severity"] in ["low", "info"]])
        self.results["summary"]["passed"] = max(0, len(issues) - self.results["summary"]["failed"])
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        self.results["pass"] = self.results["summary"]["failed"] == 0
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"# Enhanced Root Layer Validation Report",
            f"**Validation ID**: {self.results['validation_id']}",
            f"**Timestamp**: {self.results['timestamp']}",
            f"**Workspace**: {self.results['workspace']}",
            "",
            "## ğŸ“Š Summary",
            f"- **Total Checks**: {self.results['summary']['total_checks']}",
            f"- **Passed**: {self.results['summary']['passed']}",
            f"- **Failed**: {self.results['summary']['failed']}",
            f"- **Warnings**: {self.results['summary']['warnings']}",
            f"- **Info**: {self.results['summary']['info']}",
            f"- **Status**: {'âœ… PASSED' if self.results['pass'] else 'âŒ FAILED'}",
            "",
            "## ğŸš¨ Critical & High Issues"
        ]
        
        critical_high_issues = [i for i in issues if i["severity"] in ["critical", "high"]]
        if critical_high_issues:
            for issue in critical_high_issues:
                report_lines.extend([
                    f"### {issue['severity'].upper()}: {issue['message']}",
                    f"- **File**: `{issue['file_path']}`",
                    f"- **Category**: {issue['category']}",
                    f"- **Suggestion**: {issue['suggestion'] or 'No suggestion available'}",
                    f"- **Auto-fixable**: {'Yes' if issue['auto_fixable'] else 'No'}",
                    ""
                ])
        else:
            report_lines.append("âœ… No critical or high issues found!")
        
        report_lines.extend([
            "",
            "## âš ï¸ Medium Issues"
        ])
        
        medium_issues = [i for i in issues if i["severity"] == "medium"]
        if medium_issues:
            for issue in medium_issues:
                report_lines.extend([
                    f"### {issue['message']}",
                    f"- **File**: `{issue['file_path']}`",
                    f"- **Category**: {issue['category']}",
                    f"- **Suggestion**: {issue['suggestion'] or 'No suggestion available'}",
                    ""
                ])
        else:
            report_lines.append("âœ… No medium issues found!")
        
        report_lines.extend([
            "",
            "## ğŸ“ˆ File Metrics",
            ""
        ])
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        sorted_metrics = sorted(metrics.items(), key=lambda x: x[1].quality_score, reverse=True)
        
        for file_path, metric in sorted_metrics:
            status = "ğŸŸ¢" if metric.quality_score >= 90 else "ğŸŸ¡" if metric.quality_score >= 70 else "ğŸ”´"
            report_lines.extend([
                f"{status} **{metric.file_path}** ({metric.file_type})",
                f"- Quality Score: {metric.quality_score}/100",
                f"- Size: {metric.size_kb} KB",
                f"- Entities: {metric.entity_count}",
                f"- References: {metric.reference_count}",
                f"- Dependencies: {metric.dependency_count}",
                f"- Complexity: {metric.complexity_score}",
                ""
            ])
        
        report_lines.extend([
            "",
            "## ğŸ”§ Auto-fixable Issues",
            ""
        ])
        
        auto_fixable_issues = [i for i in issues if i["auto_fixable"]]
        if auto_fixable_issues:
            for issue in auto_fixable_issues:
                report_lines.extend([
                    f"- `{issue['file_path']}`: {issue['message']}",
                    f"  **Fix**: {issue['suggestion']}",
                    ""
                ])
        else:
            report_lines.append("âœ… No auto-fixable issues found!")
        
        return "\n".join(report_lines)
    
    def save_results(self) -> None:
        """ä¿å­˜éªŒè¯ç»“æœ"""
        # ä¿å­˜MarkdownæŠ¥å‘Š
        report_content = self.generate_enhanced_report()
        report_path = self.evidence_root / "enhanced_validation_report.md"
        report_path.write_text(report_content, encoding="utf-8")
        
        # ä¿å­˜JSONç»“æœ
        json_path = self.evidence_root / "enhanced_validation_results.json"
        json_path.write_text(
            json.dumps(self.results, indent=2, ensure_ascii=False),
            encoding="utf-8"
        )
        
        print(f"Enhanced validation report saved: {report_path}")
        print(f"Enhanced validation results saved: {json_path}")
    
    # è¾…åŠ©æ–¹æ³•
    def _load_validation_schemas(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½éªŒè¯æ¨¡å¼"""
        return {
            "config": {
                "required_fields": ["version"],
                "fields": {
                    "version": {"type": "string"},
                    "created": {"type": "string"},
                    "updated": {"type": "string"}
                }
            },
            "spec": {
                "required_fields": ["version", "rules"],
                "fields": {
                    "version": {"type": "string"},
                    "rules": {"type": "array"}
                }
            },
            "registry": {
                "required_fields": ["version", "entries"],
                "fields": {
                    "version": {"type": "string"},
                    "entries": {"type": "array"}
                }
            }
        }
    
    def _determine_file_type(self, filename: str) -> str:
        """ç¡®å®šæ–‡ä»¶ç±»å‹"""
        if "config" in filename or filename.startswith("root.") and not any(x in filename for x in ["specs.", "registry.", "gates."]):
            return "config"
        elif "specs." in filename:
            return "spec"
        elif "registry." in filename:
            return "registry"
        elif "gates." in filename:
            return "gates"
        else:
            return "other"
    
    def _validate_field_type(self, value: Any, expected_type: str) -> bool:
        """éªŒè¯å­—æ®µç±»å‹"""
        type_map = {
            "string": str,
            "number": (int, float),
            "boolean": bool,
            "array": list,
            "object": dict
        }
        expected_python_type = type_map.get(expected_type)
        return expected_python_type and isinstance(value, expected_python_type)
    
    def _extract_urns(self, content: Dict[str, Any]) -> Set[str]:
        """æå–URNå¼•ç”¨"""
        urns = set()
        content_str = json.dumps(content, ensure_ascii=False)
        urn_pattern = r'urn:[:\w\-.]+'
        urns.update(re.findall(urn_pattern, content_str))
        return urns
    
    def _extract_file_references(self, content: Dict[str, Any]) -> Set[str]:
        """æå–æ–‡ä»¶å¼•ç”¨"""
        references = set()
        content_str = json.dumps(content, ensure_ascii=False)
        
        # åŒ¹é…æ–‡ä»¶è·¯å¾„æ¨¡å¼
        file_patterns = [
            r'[\w\-./]+\.(yaml|yml|md|py|sh)',
            r'controlplane/[\w\-./]+',
            r'workspace/[\w\-./]+'
        ]
        
        for pattern in file_patterns:
            references.update(re.findall(pattern, content_str))
        
        return references
    
    def _extract_dependencies(self, content: Dict[str, Any]) -> Set[str]:
        """æå–ä¾èµ–å…³ç³»"""
        dependencies = set()
        
        # ä»depends_onå­—æ®µæå–
        if "depends_on" in content:
            if isinstance(content["depends_on"], list):
                dependencies.update(content["depends_on"])
            elif isinstance(content["depends_on"], str):
                dependencies.add(content["depends_on"])
        
        # ä»importså­—æ®µæå–
        if "imports" in content:
            if isinstance(content["imports"], list):
                dependencies.update(content["imports"])
        
        return dependencies
    
    def _extract_entities(self, content: Dict[str, Any]) -> Set[str]:
        """æå–å®ä½“"""
        entities = set()
        
        # æå–é”®å
        entities.update(content.keys())
        
        # é€’å½’æå–åµŒå¥—å®ä½“
        def extract_nested(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    entities.add(key)
                    extract_nested(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_nested(item)
        
        extract_nested(content)
        return entities
    
    def _calculate_complexity(self, content: Dict[str, Any]) -> int:
        """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
        complexity = 0
        
        # åŸºäºåµŒå¥—æ·±åº¦
        def calculate_depth(obj, current_depth=0):
            if isinstance(obj, dict):
                return max([calculate_depth(v, current_depth + 1) for v in obj.values()])
            elif isinstance(obj, list):
                return max([calculate_depth(item, current_depth + 1) for item in obj])
            else:
                return current_depth
        
        depth = calculate_depth(content)
        complexity += depth * 10
        
        # åŸºäºå¯¹è±¡æ•°é‡
        def count_objects(obj):
            if isinstance(obj, dict):
                return 1 + sum(count_objects(v) for v in obj.values())
            elif isinstance(obj, list):
                return sum(count_objects(item) for item in obj)
            else:
                return 0
        
        object_count = count_objects(content)
        complexity += object_count * 5
        
        # åŸºäºæ•°ç»„é•¿åº¦
        def count_array_items(obj):
            if isinstance(obj, list):
                return len(obj) + sum(count_array_items(item) for item in obj)
            elif isinstance(obj, dict):
                return sum(count_array_items(v) for v in obj.values())
            else:
                return 0
        
        array_items = count_array_items(content)
        complexity += array_items * 2
        
        return min(complexity, 100)  # é™åˆ¶æœ€å¤§å€¼
    
    def _calculate_quality_score(self, content: Dict[str, Any], filename: str) -> int:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        score = 100
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["version"]
        for field in required_fields:
            if field not in content:
                score -= 20
        
        # æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§
        doc_fields = ["description", "created", "updated"]
        for field in doc_fields:
            if field not in content:
                score -= 5
        
        # æ£€æŸ¥å‘½åè§„èŒƒ
        if not self._validate_naming_convention(filename, content):
            score -= 10
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if not self._validate_data_integrity(content):
            score -= 15
        
        return max(score, 0)
    
    def _validate_naming_convention(self, filename: str, content: Dict[str, Any]) -> bool:
        """éªŒè¯å‘½åè§„èŒƒ"""
        # ç®€å•çš„å‘½åè§„èŒƒæ£€æŸ¥
        if filename.startswith("root."):
            return True
        return False
    
    def _validate_data_integrity(self, content: Dict[str, Any]) -> bool:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
        def check_empty(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if value is None or value == "":
                        return False
                    if not check_empty(value):
                        return False
            elif isinstance(obj, list):
                for item in obj:
                    if not check_empty(item):
                        return False
            return True
        
        return check_empty(content)
    
    def _find_registry_files_for_urn(self, urn: str) -> List[str]:
        """æŸ¥æ‰¾URNå¯¹åº”çš„æ³¨å†Œè¡¨æ–‡ä»¶"""
        related_files = []
        
        for registry_file in self.registry_root.glob("root.registry.*.yaml"):
            content = self._load_yaml(registry_file)
            if content and "entries" in content:
                for entry in content["entries"]:
                    if entry.get("urn") == urn:
                        related_files.append(str(registry_file.relative_to(self.workspace_root)))
                        break
        
        return related_files


def main():
    """ä¸»å‡½æ•°"""
    validator = EnhancedRootValidator()
    validator.save_results()
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print(f"\n=== Enhanced Validation Summary ===")
    print(f"Total Checks: {validator.results['summary']['total_checks']}")
    print(f"Passed: {validator.results['summary']['passed']}")
    print(f"Failed: {validator.results['summary']['failed']}")
    print(f"Warnings: {validator.results['summary']['warnings']}")
    print(f"Status: {'PASSED' if validator.results['pass'] else 'FAILED'}")
    
    return 0 if validator.results['pass'] else 1


if __name__ == "__main__":
    sys.exit(main())